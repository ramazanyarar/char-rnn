{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "colab_type": "code",
    "id": "x6MbA_ySzkLE",
    "outputId": "b55a71a1-fffc-43c6-ab55-114f4fe924e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w': 0, 'i': 1, 'e': 2, 'c': 3, 'f': 4, 'o': 5, ' ': 6, 's': 7, 'p': 8, 'r': 9, 'd': 10, 'x': 11, 'h': 12, \"'\": 13, 'z': 14, 'b': 15, 'k': 16, '’': 17, 'a': 18, 't': 19, 'm': 20, 'u': 21, 'v': 22, 'n': 23, 'g': 24, 'j': 25, 'l': 26, 'q': 27, 'y': 28}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "854107"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk.tokenize as d\n",
    "import string\n",
    "# Data preprocessing\n",
    "#https://github.com/ryanmcdermott/trump-speeches\n",
    "with open ('speeches.txt','r') as file:\n",
    "    data=file.read()\n",
    "# text preprocessing\n",
    "table = str.maketrans({key: None for key in string.punctuation})\n",
    "table = str.maketrans({key: None for key in string.digits})\n",
    "table = str.maketrans({key: None for key in '\\n?.!/;:[!@#$]\\ufeff=–()..._…\"&é%”-—‘“0123456789,'})\n",
    "data = data.translate(table).lower()  \n",
    "#tokenize every character\n",
    "data_tk=list(data)\n",
    "#get distinct character tuple\n",
    "characters=set(data_tk)\n",
    "char_index = [x[0] for x in characters]\n",
    "#print(char_index)\n",
    "char_to_index = {v:i for i,v in enumerate(characters)}\n",
    "ix_to_char = { i:ch for i,ch in enumerate(characters) }\n",
    "#print(char_to_index)\n",
    "data_index = [char_to_index[char] for char in data]\n",
    "print(char_to_index)\n",
    "#print(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Z2CoNAgzkLM"
   },
   "outputs": [],
   "source": [
    "\n",
    "#RNN Architecture\n",
    "\n",
    "unit_size=100\n",
    "step_size=25\n",
    "learning_rate=1e-1\n",
    "\n",
    "#weight inilitiaze\n",
    "Whx =np.random.randn(unit_size,len(characters))*0.01\n",
    "Whh =np.random.randn(unit_size,unit_size)*0.01\n",
    "Why =np.random.randn(len(characters),unit_size)*0.01\n",
    "Bh =np.zeros(shape=(unit_size,1))\n",
    "By =np.zeros(shape=(len(characters),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9cWy8ZJrzkLQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##forward propagation and loss\n",
    "\n",
    "\n",
    "def forward_prop(inputs,targets,hprevious):\n",
    "\n",
    "#use  h and y arrays to keep functions (tanh, softmax) outputs\n",
    "    y=np.zeros(shape=(step_size,len(characters)))\n",
    "    h= np.zeros(( step_size + 1,unit_size ))\n",
    "    hprevious=np.squeeze(hprevious).shape\n",
    "    h[-1] =np.copy(hprevious)\n",
    "    total_loss=0\n",
    "\n",
    "#one hot encoding for inputs\n",
    "    x=np.zeros(shape=(len(inputs),len(characters)))\n",
    "\n",
    "    for i in range(len(inputs)):\n",
    "    #print(i)\n",
    "        x[i][inputs[i]]=1\n",
    "\n",
    "    for t in range(step_size):\n",
    "    \n",
    "        h[t]=np.tanh(np.dot(Whx,x[t])+np.dot(Whh,h[t-1]))#+Bh[:,0])#+Bh[:,0])\n",
    "        y_linear=np.dot(Why,h[t,])+By[:,0] # linear part of softmax func\n",
    "        y[t]=np.exp(y_linear - np.max(y_linear))/(np.exp(y_linear - np.max(y_linear)).sum()) # softmax func\n",
    "        total_loss += -np.log(y[t][targets[t]]) # Calculate loss, loss equal to negative log sum of correct index character's probability \n",
    "\n",
    "\n",
    "\n",
    "##Back Propogation\n",
    "#create array to keep parameters derivatives\n",
    "    dLdWhx=np.zeros(Whx.shape) # loss derivatives respect to Whx, to find Whx effect to loss\n",
    "    dLdWhh=np.zeros(Whh.shape)\n",
    "    dLdWhy=np.zeros(Why.shape)\n",
    "    dLdBy=np.zeros(By.shape)\n",
    "    dLdBh=np.zeros(Bh.shape)\n",
    "    dLdHp=np.zeros(h[0].shape)  # to add previous layer memory, \n",
    "\n",
    "\n",
    "    for  i in reversed (range(len(inputs))):               \n",
    "        dy=np.copy(y[i]) # copy t th probability \n",
    "        dy=np.reshape(dy,(len(characters),1))\n",
    "        dy[targets[i]]-=1    # derivatives respect to loss only depend on correct character prob., so change prob to equals 1\n",
    "        #print(dy[targets[i]])\n",
    "        dLdWhy+=np.dot(dy,np.reshape(h[i],(1,unit_size)))\n",
    "        dLdBy+=dy\n",
    "        dhl= np.dot(Why.T,dy)+np.expand_dims(dLdHp, axis=1) # hidden layer back propagation\n",
    "        dact = (1 - np.expand_dims(h[i], axis=1) * np.expand_dims(h[i], axis=1)) * dhl # backprob tanh activation func\n",
    "        dLdBh+=dact\n",
    "        dLdWhx+=np.dot(dact,np.expand_dims(x[i], axis=0))\n",
    "        dLdWhh += np.dot(dact,np.expand_dims(h[i-1], axis=0))\n",
    "        dhnext = np.dot(Whh.T, dact)\n",
    "    for dparam in [dLdWhx, dLdWhh, dLdWhy, dLdBy, dLdBh]:\n",
    "      np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    return total_loss,dLdWhx,dLdWhh,dLdWhy,dLdBy,dLdBh,h[len(inputs)-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hzBnUnaOzkLa"
   },
   "outputs": [],
   "source": [
    "def sample(hprevious, index, count):\n",
    "    x = np.zeros((len(characters), 1))\n",
    "    x[index] = 1\n",
    "    predict = []\n",
    "    hprevious=np.reshape(hprevious,(unit_size,1))\n",
    "    for t in range(count):\n",
    "        h = np.tanh(np.dot(Whx, x) + np.dot(Whh, hprevious)+Bh)\n",
    "        y = np.dot(Why, h)\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        predicted = np.random.choice(range(len(characters)), p=p.ravel())\n",
    "        x = np.zeros((len(characters), 1))\n",
    "        x[predicted] = 1\n",
    "        predict.append(predicted)\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4933
    },
    "colab_type": "code",
    "id": "c5SqRuROzkLf",
    "outputId": "72486266-e9d1-408f-bfe5-95693a2f4a2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss  39.733546810718984 n 0 s 26\n",
      "isofoki’sofofofinofofofofoupevinousabofofifoudofofofofofofofofofofougofisoveva nofoupusoloupevani’soxalyoffofrimalexrave\n",
      "\n",
      "\n",
      "Total loss  55.635727550105486 n 5000 s 130026\n",
      "higffow w p angw ackep’dy jug w yigngngry ala w wedy ppp w blang’vedy w ca w sighigdemidy’vep y w wigongakedelllud w ex \n",
      "\n",
      "\n",
      "Total loss  38.14690021369081 n 10000 s 260026\n",
      "on’rolasove’roxawaid holaly’landly’ten’lixay’chenono’ly k routilalisole’lyon’ly’lay’la’lalalalhinanonomanilay’lancile’la\n",
      "\n",
      "\n",
      "Total loss  43.106595519326156 n 15000 s 390026\n",
      "yofofoknofokli’sprabyosexexanoboofofofofofofofofofofofoknagofori bi’ni’sofi’sofagoknapofofokinofovabevexbyofofofofofaspe\n",
      "\n",
      "\n",
      "Total loss  26.113111470976218 n 20000 s 520026\n",
      "o’pesat chericto’st icechowenachicwerele’sto’mobengucccorpo'mo fy’shin miceaccofy’sict ichoweteten o’sto’rcat wo’stowhic\n",
      "\n",
      "\n",
      "Total loss  38.87687381555088 n 25000 s 650026\n",
      "moowofffyivicewe waqboto vice fobowace’ve’vice touchytobk gicbvotowo jice mimicowe ce fffuce’cetikebobaly we icid’mifhiv\n",
      "\n",
      "\n",
      "Total loss  57.2025331376434 n 30000 s 780026\n",
      "upofofovofrusmofofupexofa yorouprwexprofohanexi’spevonexotofupofofokevexabexofofacofanoupanowhi fofofofuplexranofopemane\n",
      "\n",
      "\n",
      "Total loss  51.605064134838834 n 35000 s 60008\n",
      "cfhiceme’tice ame palace’vice’vompake oby ge he bagy be me athid jor ytobeshe odeme’tybovey weriiceybequmedcybod we’tiap\n",
      "\n",
      "\n",
      "Total loss  57.528200404520156 n 40000 s 190008\n",
      "upri’srabamofquprofiscthabimoqunfofofachequpa'vexi’sapunexi’sla govoupexmyounofonofofoxi’spofmofoqunequmouscovisfa’thish\n",
      "\n",
      "\n",
      "Total loss  35.98817928170968 n 45000 s 320008\n",
      "zy’ro wily’roray’me’xswe’rourororororonuromay’rowe’roromouly’moralily’rouy’ruroroly oro ilay’rorororouy’n’xay’rorourily’\n",
      "\n",
      "\n",
      "Total loss  43.7967752490268 n 50000 s 450008\n",
      "gnovovakmopak childy abaks ilwaky cchikik obak weagichiovicchoucondovajaky hadxork jofjonouwhixwnovas jond gakachoonovef\n",
      "\n",
      "\n",
      "Total loss  61.4486401013434 n 55000 s 580008\n",
      "rewaywexayicepy’dayo xbaco o bayo’vewecayotivefaybuybameaxo caybayowe’recoyowazeo pyfixecbunoweco do p co o eay visido w\n",
      "\n",
      "\n",
      "Total loss  45.89563872285146 n 60000 s 710008\n",
      "’resequkevekesepire qut’rerepe’vi’rukephepppppp g’lkepelksekedpppsep blkeppepppp’requkeppepp g’m p opedy’rekequblffff pp\n",
      "\n",
      "\n",
      "Total loss  69.70977426677081 n 65000 s 840008\n",
      "ppoowaxyowaysmoumogowedoyowags plys toway ppruqupaguqlyoowedlyoway wiowobowed q amad t rizay'vaived qagun waburoway peda\n",
      "\n",
      "\n",
      "Total loss  36.7624398137589 n 70000 s 119990\n",
      "i’squpofofonofofofofifovexofoxofuplova’sanoyofofroupexabexatoupexryofoki’tha’sanouscabuplexacabexi’spequsouparexofowofof\n",
      "\n",
      "\n",
      "Total loss  36.40275035948245 n 75000 s 249990\n",
      "yofofofofousi’scaluoweqanoxofrofofounofexousofrexi’susofovexpexi soupupaburwhequsopagofi’shalokifofofofousofoxexri’thi’t\n",
      "\n",
      "\n",
      "Total loss  44.73384480463256 n 80000 s 379990\n",
      "gwh s f caly ig cth bigaler y d cerzerecjothy bewacig’re hewe acry’veo ly usy’s i’thilyodwawead q’s q’s f jad ps ferer f\n",
      "\n",
      "\n",
      "Total loss  36.159309568997415 n 85000 s 509990\n",
      "upupexnexlalofofaloupnevexrexbyousasarexpexpupafofofonexagoyexnexrexrexexrusousapowarupofithasofofadoprunonifi’sapexalof\n",
      "\n",
      "\n",
      "Total loss  53.959317377580064 n 90000 s 639990\n",
      "s at fas’n’s’s’facabecabacthhcthvi’caizad jantocththbmbuved mes’fevifely’mexzy’m ca facovi’s’nced ks wepqupazy’s’ss fuve\n",
      "\n",
      "\n",
      "Total loss  53.131761888823966 n 95000 s 769990\n",
      "qucofonofunofamofofi’sanofofofasexmanofofofuplomexnupranadi whagryofofupofupi’spanokexabunoknovonofrexhana ’supupexanequ\n",
      "\n",
      "\n",
      "Total loss  58.573251962928104 n 100000 s 49972\n",
      "'sppsppppdywsppppp w'vedckeduppri ndop owakcuppff medhspppppopedoppppspd’mekch wd mpppf wwendqup ckesppppbllkedw omesppo\n",
      "\n",
      "\n",
      "Total loss  37.97138815543013 n 105000 s 179972\n",
      "pp w w w w bldy’vi’vy’ves bci’mp wi’v’ves gg’my’theveybug’vegs’mig’vemy’vesssi’my'vevem ga hbveyblywi’vijveyg jg’vyeves \n",
      "\n",
      "\n",
      "Total loss  32.50297133895803 n 110000 s 309972\n",
      "qupourivayoupay’s wowedexadas h qus cexaly’stlan’veved’s kexaved’s kyourizex y pwawevesoupumowedevous whe weded'aquced’s\n",
      "\n",
      "\n",
      "Total loss  41.12223314684282 n 115000 s 439972\n",
      "ico galico wefo mbudesedizitscaly’so’esly’sely wole sesal ico jomioburexececeul wo cicalfo’sexly’soly’sobush vefalffalic\n",
      "\n",
      "\n",
      "Total loss  54.31732780058726 n 120000 s 569972\n",
      " ff k’s buselustiboumysbuss ccjuk’sh vekn tiq’ppppmystick’sffhstemamestry fusby’rt’rywhstas bmevemedi’ststok’stekext’s y\n",
      "\n",
      "\n",
      "Total loss  42.44992554391265 n 125000 s 699972\n",
      "s owamumikeyweacowaqumokeaducbeycucoumemy’memoubaqumoweacedeyojumesend mickemouzemoweamicheuchicumowemeyweaqumimicadefiv\n",
      "\n",
      "\n",
      "Total loss  26.958458523879294 n 130000 s 829972\n",
      "edgatixbct cppppppppppppppppppppppps vedmppmstrnicts ppppppp wedgrzs bogwsk fredfffffdssyodbody psyat kcbodfgeddmandgred\n",
      "\n",
      "\n",
      "Total loss  57.656433684425 n 135000 s 109954\n",
      "n’vavavie's g’vowad’ve qudd’ms h g avie’s coulie’se fuly t ojen’ve’mif taz’s’vaz bume’ve t h cowe ag be’s squsqupsn’s gd\n",
      "\n",
      "\n",
      "Total loss  53.18467810033194 n 140000 s 239954\n",
      "obx pode to’sorobe’stoii’sowowesiooriodrore joron joucorowi’s face izasionoobadole wanobioxy we me woce iowe wcooobco’ro\n",
      "\n",
      "\n",
      "Total loss  30.808333850935334 n 145000 s 369954\n",
      "anl wilas choro ly’x wonhaworoubowuswanouthomou’ro’lay kay’xite’son’re’sowole’roroutouje’roulalache’rouhe’rocthe’rorout \n",
      "\n",
      "\n",
      "Total loss  43.58572679478638 n 150000 s 499954\n",
      "hexrofofofofevexnexloinoupovexrwocofabuthalexnexrexrofexrulexwarinalxlofofapexloknexaloupi’salofropexrounarexnexloupowis\n",
      "\n",
      "\n",
      "Total loss  52.37138437402404 n 155000 s 629954\n",
      "wenmaybecomixaixomivechonomecupereay’me’reaumecoupeco omailewereruchoupeoucobeco cway’mobecoxaxcweweemo fchoumayupo aupe\n",
      "\n",
      "\n",
      "Total loss  33.41994289680947 n 160000 s 759954\n",
      "ityig’mpppp iguguqughigongwesughilug’langhiquvicthengrybdkevesilkepsmiwevevellalivokesevty  w wawyw mppp wepw angwy malu\n",
      "\n",
      "\n",
      "Total loss  70.09793370655535 n 165000 s 39936\n",
      "’voupoupefawefay d'molyicugequs mouy xep trsewas bedaqumaswen’vidyouvecagakeaym fan’vobuvay wayqucoperyoupakavipous heco\n",
      "\n",
      "\n",
      "Total loss  58.30424439874431 n 170000 s 169936\n",
      "i’nugazousofofofi’nabexabexexi’sofanofocexexexevexrexi’thonophagonoupryocagounonuprevisquganofori’sofofevi’chalomugexeva\n",
      "\n",
      "\n",
      "Total loss  48.635263667802 n 175000 s 299936\n",
      "h cobonmere tutowowoce’t p’storexicowhenowecowe’ plymimed’twredfutfuto’tt’moched’tuth m whe’mobawonme’sorus’s jevowewibu\n",
      "\n",
      "\n",
      "Total loss  75.80738081557362 n 180000 s 429936\n",
      "owaqumythavibicham dize t’vikemiceytricedybavemicowave’mojytizamiksciowace funfuytiequbace ce swamicaqubkie’sfucowowe fo\n",
      "\n",
      "\n",
      "Total loss  43.1081240011316 n 185000 s 559936\n",
      "hekeviggzam aqhblllllkekedgtighzebledalcecab wep cankec hb kedp whalpp ckedblked sthevedgzighenghedighaq kedededhabllbed\n",
      "\n",
      "\n",
      "Total loss  25.058048958264354 n 190000 s 689936\n",
      "w opqudon’s f’s con’ly t’mi’m cm vof octan’sm or by w ct’spyocoxpy’m coy's teventh’cthan’s yochs joxan’s ththckin’s hs b\n",
      "\n",
      "\n",
      "Total loss  40.0403415858516 n 195000 s 819936\n",
      "rexesevepevevirep opheppre ckeckeppherefffffuchpppef wepprekevepsen’rekeveven’m blokeve t’res ppevesepsequt’vewkewepefop\n",
      "\n",
      "\n",
      "Total loss  37.00679955729943 n 200000 s 99918\n",
      "pade g’san’tucin’milie’lawos pe’ly’wolyomo wom pan’ti’pous ce’tlin’powomiowopan’ppon’mi’so’mion’von’sin’ppo py’toumo’mi’\n",
      "\n",
      "\n",
      "Total loss  30.907674444575626 n 205000 s 229918\n",
      "i'nofoforexi’seveprevabyoxamouphafoupamupofexrofefofofothafexi’thevanofofi’soxanofomathanofokvoupli’sulathi’sbunbupofofa\n",
      "\n",
      "\n",
      "Total loss  40.77030623199181 n 210000 s 359918\n",
      "t’ld spppr prviten’r w lksuprgouspprs n oguvind i’rurm w oy’ss flllinc psppsp’vevi’lyom gi’sm ouspr wad t’schrgous juvec\n",
      "\n",
      "\n",
      "Total loss  49.504175328489524 n 215000 s 489918\n",
      "hughesighe’veclcy w wevec dighesevend wanghey’vexingighinghevewhigheszesghenghigheghhecevepudengwhighenghenghesif cseghe\n",
      "\n",
      "\n",
      "Total loss  24.771582426613882 n 220000 s 619918\n",
      "medjw tow yifw w kybearyormstif w k tw klytw clyic’sow k w’sw wex wi wmed p w tw’sow jwh’sowsswa tw wedmoc jw i ’tow a t\n",
      "\n",
      "\n",
      "Total loss  39.89303243332699 n 225000 s 749918\n",
      "uswofrexexexamofrobounuxhex i’squlexinexevexoinevinabupobexrixini’stofonequsabuplofobupexi’loywonbunemowacupobupobumabac\n",
      "\n",
      "\n",
      "Total loss  49.32407835271272 n 230000 s 29900\n",
      "bumion juuybazybalegupe's ppplewazy ngowazyt buy wazizy mous wazym blemecan's thegapppleroves ve wazymen t pleeriorupe w\n",
      "\n",
      "\n",
      "Total loss  44.4324044214558 n 235000 s 159900\n",
      "lwavedmmp jowhhixipevevep whixwavepmppwaxix wh chinwhowiguw can jgdllwavep gwanwap zalw whowe’mp gus dampld wes t’mp jw \n",
      "\n",
      "\n",
      "Total loss  53.001980512355594 n 240000 s 289900\n",
      "anofi’sofouplofoxi’sharofofokexi’supofofofocthofoupanofagofonoulx pruprexamyovexi’shevoupexabupami’sousanonofofou lexpof\n",
      "\n",
      "\n",
      "Total loss  45.21173997689947 n 245000 s 419900\n",
      "oupousouqugminizazilazaququziousioniliowaquzimeniowas juniziononiziqusijuponmyoqusonizamiougupouzizizizime pounousinaquz\n",
      "\n",
      "\n",
      "Total loss  47.716958435137585 n 250000 s 549900\n",
      "s wag wagrfr’s gid f gif’s s g nz ay’s f ows jf b cks g’s fiqug bs ppps pld pps wqus ppptcks pqusex fffthag b ff pppls j\n",
      "\n",
      "\n",
      "Total loss  43.96220107796869 n 255000 s 679900\n",
      " wk pf bk cked jk w ck hk hk m p a pp ized ak q f hke p t’ved b bk r cke w hk bk hkes ck p hk chked pp wk rk p hk bk hke\n",
      "\n",
      "\n",
      "Total loss  43.166211366630485 n 260000 s 809900\n",
      "ubupaked’t’s waupivin’y od y’s fadd y’ched aaqugwn’vifp bumerowaton’san’smo’ve co’shoon’swod’techevaps fmow’so’p o’tovas\n",
      "\n",
      "\n",
      "Total loss  40.82678649482505 n 265000 s 89882\n",
      "wakevedy’mppst pmcavevengllsirrwstingokingngecilw b wfyt axy lugpppsppppppesescakinaxpppngngh whapppppppp w wsappingppme\n",
      "\n",
      "\n",
      "Total loss  48.27189617035409 n 270000 s 219882\n",
      "pwhowivaxpwhowhevwk ccowescavavaveppw w jwaljmpwhh w ppldudmmmp ncoudhowhices qudrmmbe’vave ky dlllwhepp p kympwevicowav\n",
      "\n",
      "\n",
      "Total loss  37.32038128341214 n 275000 s 349882\n",
      " wey we fheppey pe tad pes tevicwifixis ifum wefly’tfmexeviogy i pisely’vy tqum y twowadywawawevom wevofizyowevy ely iec\n",
      "\n",
      "\n",
      "Total loss  30.668617925761264 n 280000 s 479882\n",
      "uky’s who’s j’sowaceceraly’sich te chido’so o’s y’s gh g’schanai’so’so’so’s we’ly’s ly’scheraraby’suk dly’shaly’s fy che\n",
      "\n",
      "\n",
      "Total loss  58.18920552395428 n 285000 s 609882\n",
      "qushevopoxa wevi’sofokzonofofugousomevokevexi’soupofevexyofonoknusofuphaga wi’sa pachevexpevexhenoknovexi’suxnofofoknono\n",
      "\n",
      "\n",
      "Total loss  41.253299268538974 n 290000 s 739882\n",
      "umecononononoxaksonononono onononononononononononononouvechidefadefecesoutefaxebononon'tononononononononononononocefexec\n",
      "\n",
      "\n",
      "Total loss  51.30546271931924 n 295000 s 19864\n",
      "nfut’ppph qubuty’mbf’sn’mbutwepexilin an’s wecabowenagony’futepowg’ten’s weniwinopobopen’sspen thtt'sn'vesargut’pe w dut\n",
      "\n",
      "\n",
      "Total loss  42.47223568733155 n 300000 s 149864\n",
      "sowehe we’stouce’m bouce’s’fgybaququm’s jemeeorwececizad’wecepybaqupyofqceaqn’daq’s’davadawefbaweougajeaqutovowevaweopyb\n",
      "\n",
      "\n",
      "Total loss  40.29405118260575 n 305000 s 279864\n",
      "oxi’stofi’mumofofoxagi’tofofi jupohanexofevevexexagofofofunofocabexagousunounousthi’nomi’sovexabununexevagowacoupaprori’\n",
      "\n",
      "\n",
      "Total loss  49.298295662617086 n 310000 s 409864\n",
      "owayobayoo o webo fpybovepicchidayifoo tayifaybowakacay’s fayo vexebonoixechocidubayidwevepefifay’pecofbofayidiffaxe tro\n",
      "\n",
      "\n",
      "Total loss  41.747821656942584 n 315000 s 539864\n",
      "noute’xoron’ronoube’rowele’rowo’rorou’p’rorite’role’roly’te’sorowe’lasothe’rooro’so’re’site’ro woute’rowouncwouworowime’\n",
      "\n",
      "\n",
      "Total loss  47.58385241764103 n 320000 s 669864\n",
      "cppasi’sofupabusi’sofofofunovabushemvexanofrofofofofaboki’sexovevofofri’sofupounovi’shofoxofonofoxloxagri’mokyofofoupran\n",
      "\n",
      "\n",
      "Total loss  39.47875412696136 n 325000 s 799864\n",
      "houcinsuweses fy fy'thatisuchis wey mouss oousty o fysichoucin’shy myousruchy’st we fy’s’styouzy thouzy t’manhy ouy weyo\n",
      "\n",
      "\n",
      "Total loss  39.27378968162499 n 330000 s 79846\n",
      " gulllk whbveps cavepavep wevwavepp walllllldllllpllllwak’mppwkugchowguf nt cowgp k dchawigowmp ldwhe cowcowavcor wn’ves\n",
      "\n",
      "\n",
      "Total loss  42.319674980778096 n 335000 s 209846\n",
      "bejestawedy thiviviowhbowe waut t miowhbobvixe wefut fugivacedize jawawhbowedcawbout outce’ly’s aly’s i’s pe’s caqu pawh\n",
      "\n",
      "\n",
      "Total loss  32.26009262283761 n 340000 s 339846\n",
      " pice buqusowic wan re h tay ze’soxe mice hy’s ffowy m we way ch st ce icays wan p we pay’ssom owe sowy t so t ps j’s r \n",
      "\n",
      "\n",
      "Total loss  37.95495494442387 n 345000 s 469846\n",
      "whonowexbucalabyofofofomi’sova fofofi’mofonofri’sthagri’sexa whitofofofonanofofobupofofusofofusmudofofqumupofofi’soforre\n",
      "\n",
      "\n",
      "Total loss  58.068701161152106 n 350000 s 599846\n",
      "shh juplingoplllingngnescswevit’s bupey in fupvesbupf grfupfodintttt’sttingn bgalit’sa’mblling’swedncangnglesdqwgngngw t\n",
      "\n",
      "\n",
      "Total loss  41.336435576700865 n 355000 s 729846\n",
      "ed guppppppppppppplkeppp gicskewkbld ked keppppppplkewke’m’sudedesked’med w kecckelkeddd ke’dgppplkesm gke’m ccckedd lke\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Parameters upgrade, ı use Adagrad upgrade\n",
    "# variables for previous gradient\n",
    "\n",
    "pdLdWhx=np.zeros(Whx.shape) \n",
    "pdLdWhh=np.zeros(Whh.shape)\n",
    "pdLdWhy=np.zeros(Why.shape)\n",
    "pdLdBy=np.zeros(By.shape)\n",
    "pdLdBh=np.zeros(Bh.shape)\n",
    "\n",
    "#initial parameters \n",
    "    \n",
    "#inputs=data_index[20:45]\n",
    "#targets=data_index[21:46]  \n",
    "unit_size=100\n",
    "step_size=25\n",
    "learning_rate=1e-2\n",
    "hprevious=np.zeros((unit_size,1))\n",
    "\n",
    "\n",
    "s,n = 0,0 # s start index for inputs, \n",
    "\n",
    "\n",
    "for i in range(1000000):\n",
    "\n",
    "\n",
    "    if s>=850000:\n",
    "       s=0\n",
    "\n",
    "    inputs=data_index[s:s+step_size]\n",
    "    targets=data_index[s+1:s+step_size+1]    \n",
    "    s+=step_size+1\n",
    "  \n",
    "    \n",
    "    total_loss,dLdWhx,dLdWhh,dLdWhy,dLdBy,dLdBh,hprevious=forward_prop(inputs,targets,hprevious)\n",
    "    \n",
    "    \n",
    "    if n%5000==0:\n",
    "      print('Total loss ', total_loss,'n',n,'s',s)\n",
    "      a=sample(hprevious,inputs[0],120)\n",
    "      print(\"\".join([ix_to_char[ix] for ix in a]))\n",
    "      print('\\n')  \n",
    "\n",
    "# Adagrad upgrade for paramaeters\n",
    "\n",
    "    for parameters, dparam, pdparam in zip( [Whx, Whh, Why, Bh, By], \n",
    "                                            [dLdWhx, dLdWhh, dLdWhy, dLdBh, dLdBy], \n",
    "                                            [pdLdWhx, pdLdWhh, pdLdWhy, pdLdBh, pdLdBy]):\n",
    "        pdparam += dparam * dparam\n",
    "        parameters += -learning_rate * dparam / np.sqrt(pdparam + 1e-8) # \n",
    "        \n",
    "    n=n+1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZPcrwmmkECnn"
   },
   "source": [
    "## Results\n",
    "\n",
    "I run my model 100 hidden unit  , 1e-1 learning rate and 25 charachter  memory . Total loss value  decreased 55 to 25 levels after  but model didn't provide reasonable text sample\n",
    "\n",
    "Examples:\n",
    "\n",
    "**Total loss  39.733546810718984 , Iteration 0**\n",
    "\n",
    "\"isofoki’sofofofinofofofofoupevinousabofofifoudofofofofofofofofofofougofisoveva nofoupusoloupevani’soxalyoffofrimalexrave \"\n",
    "\n",
    "\n",
    "**Total loss  30.808333850935334 Iteration 145000**\n",
    "\n",
    "anl wilas choro ly’x wonhaworoubowuswanouthomou’ro’lay kay’xite’son’re’sowole’roroutouje’roulalache’rouhe’rocthe’rorout \n",
    "\n",
    "\n",
    "**Total loss  24.771582426613882 Iteration 220000**\n",
    "\n",
    "\"medjw tow yifw w kybearyormstif w k tw klytw clyic’sow k w’sw wex wi wmed p w tw’sow jwh’sowsswa tw wedmoc jw i ’tow a t\"\n",
    "\n",
    "**Total loss  32.26009262283761  Iteration 340000**\n",
    " \n",
    " \"pice buqusowic wan re h tay ze’soxe mice hy’s ffowy m we way ch st ce icays wan p we pay’ssom owe sowy t so t ps j’s r \"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Homework2.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
